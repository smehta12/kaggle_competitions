{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is providing the solution for the [Housing Prices prediction](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) competition. This notebook shows feature engineering data cleanup and the modeling. The link to on the kaggle for this notebook is https://www.kaggle.com/smehta12/house-values-prediction-top-20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "76c653cafd7648f16a585d4f230b90e72617903d"
   },
   "source": [
    "The following kernel does some of the simple but essential steps to perform the Regression Analysis. By doing the it, it gives nice performence which puts the kernel in top 20% with the score of 0.12023 at the time of submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "681109064c081eceaa74faf96c78ef253df5aab4"
   },
   "source": [
    "## Data Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-output": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(r\"../input/train.csv\")\n",
    "df_test = pd.read_csv(r\"../input/test.csv\")\n",
    "pd.set_option('display.width', 700)\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0d39cd8b2eb73e01198bdfc361964aa6d9ccbf94"
   },
   "outputs": [],
   "source": [
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fcdb8118f11857a28dc53fed2c551965968f0621"
   },
   "outputs": [],
   "source": [
    "# Size of data before any operations:\n",
    "# Here one less col in df_test becoz SalePrice is missing.\n",
    "print(\"The train data shape : {} \".format(df_train.shape))\n",
    "print(\"The test data shape : {} \".format(df_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b31ae802b6affd6c2685c1cd8c2f2d922d72b307"
   },
   "source": [
    "## Primary Exploration of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c8378ba863a4eecb10970b93b70e148fba42dee2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_train.dtypes)\n",
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ad9f34f8a0ca4407b45a5c4942888ba8c4bdd35d"
   },
   "source": [
    "## Check for Nans/NA/Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "95b05138bd0657d8a1f5cad3a2eedf57c43f5244"
   },
   "outputs": [],
   "source": [
    "# Remove Id column. It doesn't contribute to the price prediction\n",
    "df_train.drop(\"Id\", inplace=True, axis=1)\n",
    "df_test.drop(\"Id\", inplace=True, axis=1)\n",
    "\n",
    "print(\"The train data shape after removing Id col: {} \".format(df_train.shape))\n",
    "print(\"The test data shape after removing Id col: {} \".format(df_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "437417266fb9b57861bddf68f6b6999f7c4c034d"
   },
   "outputs": [],
   "source": [
    "# Remove all the rows and columns from the data which has all values Nans/Empty cells\n",
    "df_train.dropna(axis=1, how=\"all\", inplace=True)\n",
    "df_train.dropna(axis=0, how=\"all\", inplace=True)\n",
    "\n",
    "df_test.dropna(axis=1, how=\"all\", inplace=True)\n",
    "df_test.dropna(axis=0, how=\"all\", inplace=True)\n",
    "\n",
    "print(\"The train data shape after removing all col and rows with Nans : {} \".format(df_train.shape))\n",
    "print(\"The test data shape after removing all col and rows with Nans: {} \".format(df_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cdf1bf539254c7795ea2ab6dd0592fee16ec67e5"
   },
   "outputs": [],
   "source": [
    "# Find out the frequency of nulls in the columns\n",
    "\n",
    "# For training Data\n",
    "count_nans = len(df_train) - df_train.count()\n",
    "df_count_nans = count_nans.to_frame()\n",
    "df_count_nans.columns=[\"train_nan_count\"]\n",
    "df_count_nans[\"%_train_nans\"]=(df_count_nans[\"train_nan_count\"]/df_train.shape[0]) * 100\n",
    "\n",
    "# For test data\n",
    "df_count_nans[\"test_nan_count\"] = len(df_test) - df_test.count()\n",
    "df_count_nans[\"%_test_nans\"]=(df_count_nans[\"test_nan_count\"]/df_test.shape[0]) * 100\n",
    "\n",
    "df_count_nans.sort_values(\"train_nan_count\", ascending=False, inplace=True)\n",
    "df_count_nans.query('train_nan_count > 0 or test_nan_count > 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b4acd9322b7e56afe2b805f15623a6b91443f129"
   },
   "outputs": [],
   "source": [
    "# take out the SalePrice from the train data before further processing\n",
    "y_train = df_train.SalePrice.values\n",
    "print(y_train)\n",
    "df_train.drop(\"SalePrice\", inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "655e78303d858be77d874cb3fa7d17f435b27949"
   },
   "outputs": [],
   "source": [
    "# Combining all data for the further processing\n",
    "df_all_data = pd.concat([df_train, df_test])\n",
    "df_all_data.reset_index(inplace=True, drop=True)\n",
    "print(df_all_data.shape)\n",
    "df_all_data.columns\n",
    "df_all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f12f25681fdff8f78ab054c2afcb5cb82ae408e9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# By looking at the analysis of Nans it is clear that the PoolQC, Alley, MiscFeature, Fence has many null values.\n",
    "# So those aren't contributing to the price a lot. So these columns will be dropped.\n",
    "df_all_data.drop([\"PoolQC\", \"Alley\", \"MiscFeature\", \"Fence\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f461ac2b79b8a5d1658f953f0736d17410a34136",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fill the values for the rest of the NAs\n",
    "\n",
    "# No changes in FirplaceQu. Because the NAs indicates NA=No Fireplace. Fill with None\n",
    "df_all_data[\"FireplaceQu\"].fillna(\"None\", inplace=True)\n",
    "\n",
    "# GarageCond, GarageType, GarageFinish, GarageQual. Fill with None\n",
    "df_all_data[[\"GarageCond\", \"GarageType\", \"GarageFinish\", \"GarageQual\"]] = df_all_data[[\"GarageCond\", \"GarageType\", \"GarageFinish\", \"GarageQual\"]].fillna(\"None\")\n",
    "\n",
    "# For GarageYrBlt filling with the 0. Assuming garage isn't available.\n",
    "df_all_data[\"GarageYrBlt\"].fillna(0,  inplace=True)\n",
    "\n",
    "# Fill with None/0 for basement. It is likely that basment isn't available in the houses\n",
    "df_all_data[[\"BsmtExposure\",\"BsmtFinType1\", \"BsmtFinType2\", \"BsmtCond\", \"BsmtQual\"]] = df_all_data[[\"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\", \"BsmtCond\", \"BsmtQual\"]].fillna(\"None\")\n",
    "df_all_data[[\"BsmtFullBath\", \"BsmtFinSF1\", \"BsmtFinSF2\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"BsmtHalfBath\"]] = df_all_data[[\"BsmtFullBath\", \"BsmtFinSF1\", \"BsmtFinSF2\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"BsmtHalfBath\"]].fillna(0)\n",
    "\n",
    "# MasVnrArea. Fill with 0. Prob it is not available in these houses\n",
    "df_all_data[\"MasVnrArea\"].fillna(0, inplace=True)\n",
    "df_all_data[\"MasVnrType\"].fillna(\"None\", inplace=True)\n",
    "\n",
    "# Electrical. Fill with the most common one.\n",
    "most_common = df_all_data[\"Electrical\"].value_counts().index[0]\n",
    "df_all_data[\"Electrical\"].fillna(most_common, inplace=True)\n",
    "\n",
    "# Functional. As directed in data definition, use \"Typ\" as the default val\n",
    "df_all_data[\"Functional\"].fillna(\"Typ\", inplace=True)\n",
    "\n",
    "# KitchenQual. Considering \"TA\" as default val, which is short of Typical/Average\n",
    "df_all_data[\"KitchenQual\"].fillna(\"TA\", inplace=True)\n",
    "   \n",
    "# Fill with the most common val\n",
    "most_common =  df_all_data[\"SaleType\"].value_counts().index[0]\n",
    "df_all_data[\"SaleType\"].fillna(most_common, inplace=True)\n",
    "    \n",
    "# No assumption can be made for the following columns val\n",
    "df_all_data[\"Utilities\"].fillna(\"None\", inplace=True)\n",
    "df_all_data[\"Exterior1st\"].fillna(\"None\", inplace=True)\n",
    "df_all_data[\"Exterior2nd\"].fillna(\"None\", inplace=True)\n",
    "\n",
    "# Fill with the most common val\n",
    "most_common =  df_all_data[\"MSZoning\"].value_counts().index[0]\n",
    "df_all_data[\"MSZoning\"].fillna(most_common, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8ea95867790e1fdd36d044aee46724aec3b618a1",
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Filling GarageCars per neighborhood. It is most likely that per neighborhood the car space is similar.\n",
    "grp=df_all_data.groupby(\"Neighborhood\")[\"GarageCars\"].mean()\n",
    "nan_idx = df_all_data[df_all_data[\"GarageCars\"].isnull()==True].index.tolist()\n",
    "for idx in nan_idx:\n",
    "    df_all_data.loc[idx, \"GarageCars\"] = int(round(grp.loc[df_all_data.iloc[idx][\"Neighborhood\"]]))\n",
    "\n",
    "# Filling GarageArea per neighborhood. It is most likely that per neighborhood the car space is similar.\n",
    "grp=df_all_data.groupby(\"Neighborhood\")[\"GarageArea\"].mean()\n",
    "nan_idx = df_all_data[df_all_data[\"GarageArea\"].isnull()==True].index.tolist()\n",
    "for idx in nan_idx:\n",
    "    df_all_data.loc[idx, \"GarageArea\"] = int(round(grp.loc[df_all_data.iloc[idx][\"Neighborhood\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c6a46dc1740fa0ef0553662107a7907766a9dc13",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LotFrontage: The linear feet of the street connected to the property can be based on the building types. So per building type it will be filed in with the avg.\n",
    "df_all_data[\"LotFrontage\"] = df_all_data.groupby(\"BldgType\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c1ba8aa20fb85e6dcbaf13c79219b6ea4958a7e3"
   },
   "source": [
    "## Correlation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4cc857ebfd73e2804fefd5659ec28d3c8abecbac"
   },
   "outputs": [],
   "source": [
    "# Find highly correlated features\n",
    "corr_matrix = df_all_data.corr().abs()\n",
    "plt.subplots(figsize=(15,10))\n",
    "sns.heatmap(corr_matrix, cmap=\"jet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6683bbcadf60c47512e13f9fa8483aad6d7214fe"
   },
   "outputs": [],
   "source": [
    "# Only for Trainingdata. The same relationship can be seen as above\n",
    "corr_matrix = df_train.corr().abs()\n",
    "plt.subplots(figsize=(15,10))\n",
    "sns.heatmap(corr_matrix, cmap=\"jet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0d903c45c907080ebd8c49c308e131be3ece7a3e"
   },
   "outputs": [],
   "source": [
    "# Only for Test data. The same relationship can be seen as above\n",
    "corr_matrix = df_test.corr().abs()\n",
    "plt.subplots(figsize=(15,10))\n",
    "sns.heatmap(corr_matrix, cmap=\"jet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1ba1d6c30eeb6e5555a088f4ed860b0b2a6bfe32"
   },
   "outputs": [],
   "source": [
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] >= 0.75)]\n",
    "print(to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3b8c02be40b3933f663044b7623c4396f6e26410"
   },
   "source": [
    "The description below inffered based on the data defintions and the correlation matrix.\n",
    "* GarageYrBlt and YearBlt has the high correlation. Because garages are mostly built when the houses are built.\n",
    "* GarageArea and GarageCars has very high correlation. Because if more cars can be parked, then there will be more garages space and vice versa.\n",
    "* The '1stFlrSF', 'TotalBsmtSF'  are correlated. It makes sense because usually basements are usually right below the first floor and mostly similar in size.  \n",
    "* TotRmsAbvGrd and the GrLivArea are correlated. It also makes sense because in both of the columns the basement isn't considered.\n",
    "\n",
    "So the above features will be dropped for future calculation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e27464a5f3b8f9a3fcc0e94640721ffc04be29ef",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all_data.drop(to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7e8d131b3618802c90637c18fd7b9e9a06041f4f"
   },
   "source": [
    "## Outliers\n",
    "\n",
    "Based on the orignal [Ames data defintion](http://ww2.amstat.org/publications/jse/v19n3/Decock/DataDocumentation.txt) there are outliers present into the data. Especially in the GrLivArea column. So need to find out those observations and remove it if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "da1b276f2ed589bfcf026894444388c3c3ee939f"
   },
   "outputs": [],
   "source": [
    "sns.regplot(x=df_train[\"GrLivArea\"], y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5f3a35a9cf19e7e30580daeef38f85e6ffadb733"
   },
   "source": [
    "In the above plot we can see the 2 dots in the bottom-right. They are above 4000 sq ft. but has been sold very less amount. Whereas other sellings like top right corner are sold at much more higher prices. So those 2 are outliers and should be deleted. Otherwise the models will try to capture those points and resulting overfit. The data definition also tells there're more outliers but we can keep it. By looking at the plot, it seems like all other points are following trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a89e38d0dbd5e44315bd6e45d5e7e93cf4196247",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drop_points = df_train.sort_values(by = 'GrLivArea', ascending = False)[:2][\"GrLivArea\"]\n",
    "drop_points_list = drop_points.index.tolist()\n",
    "df_all_data.drop(drop_points_list, inplace=True)\n",
    "\n",
    "# Updating the indexes for training data.\n",
    "y_train = np.delete(y_train, drop_points_list)\n",
    "df_train_last_index=df_train.shape[0]-len(drop_points_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f36b94e750b2f1aef5ce7ffc594e0060cac99600"
   },
   "source": [
    "## Categorical Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "80026ebf6701616ff63f4437d90b6bef233b84ae",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#MSSubClass=The building class\n",
    "df_all_data['MSSubClass'] = df_all_data['MSSubClass'].apply(str)\n",
    "df_all_data[\"MSSubClass\"] = LabelEncoder().fit_transform(df_all_data[\"MSSubClass\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "53fef34d387b62093e5d13f22b3c1f89a39dcd9e"
   },
   "source": [
    "## Handle the Skewness in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "64a64ec1920c9b387624efed58a32bb46b46876d"
   },
   "outputs": [],
   "source": [
    "# detect skewed columns\n",
    "skew_thresh = 0.5\n",
    "skewed = df_all_data.skew().sort_values(ascending=False)\n",
    "a=skewed[abs(skewed)>skew_thresh]\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "393803616e30e82e304ad1b9bd71657c7a39a49e"
   },
   "source": [
    "Based on above data there're some skewed data present in the data. So it will be log transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "36e6b12cba68dbfb122f4d32881ea32fbaf2ad41"
   },
   "outputs": [],
   "source": [
    "skewed_cols = skewed[abs(skewed)>skew_thresh].index.tolist()\n",
    "print(len(skewed_cols))\n",
    "print(skewed_cols)\n",
    "df_all_data[skewed_cols] = df_all_data[skewed_cols].apply(np.log1p)\n",
    "df_all_data[skewed_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6a97a184f326a4024e5b62d95c491b4f40d23c66"
   },
   "outputs": [],
   "source": [
    "# Use the one hot encoder to change the categorical data in the numeric data\n",
    "categorical_data_cols = df_all_data.select_dtypes(include=['object'])\n",
    "print(categorical_data_cols.columns.tolist())\n",
    "df_all_data = pd.get_dummies(df_all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9ce60900d53badeb8542c820a83242fe62d40f13"
   },
   "source": [
    "# Normalize the data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "79298300c471db417731543560cd0204185bc379"
   },
   "source": [
    "## Log Transformation of the SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5cf561503fba9740c0c4435d1b87e212a41d7bc4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.\n",
    "y_train = np.log1p(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "860091c1916d7a01f752267f64ae7908570f1bdb"
   },
   "source": [
    "After applying OneHotEncoder, the data is at different scales now. It needs to be normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a4224c9b462af132ff38cc79b0286b0b4aa4e1cf",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all_data = (df_all_data - df_all_data.mean()) / (df_all_data.max() - df_all_data.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f5e9b05c7a5fe6cef2aae1671e9e1bd630360247"
   },
   "outputs": [],
   "source": [
    "# Create training and test dataset after data munging\n",
    "df_tr = df_all_data.iloc[:df_train_last_index]\n",
    "df_te = df_all_data.iloc[df_train_last_index:]\n",
    "print(df_tr.shape)\n",
    "print(df_te.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c10b5727a88a3251ed082218816eba918a252458"
   },
   "source": [
    "\n",
    "## Applying Regression Algorithms\n",
    "Now is the time to apply the regression algorithms! Following algorithms are tried the ElasticNetCV gives the best result.\n",
    "\n",
    "* ElasticNet\n",
    "* Ridge\n",
    "* Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eb76538311c393216cf661071be3b6a6014ea89e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def _ApplyLinearAlgo(model_obj, df_tr, df_te, y_train):\n",
    "    model_obj.fit(df_tr, y_train)\n",
    "    y_predict = model_obj.predict(df_tr)\n",
    "    print(\"r2 score train \" + str(r2_score(y_train, y_predict)))\n",
    "    print(\"rmse score train \" + str(mean_squared_error(y_train, y_predict)))\n",
    "\n",
    "    print(df_tr.shape)\n",
    "    print(df_te.shape)\n",
    "    y_te_pred = np.expm1(model_obj.predict(df_te))\n",
    "    \n",
    "    return y_te_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "204fbc94f0780e0b97e5e25f5a04b0f7ab279728"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\")\n",
    "print(\"ElasticNetCV\")\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "lr = ElasticNetCV(alphas=[0.0001, 0.0005, 0.001, 0.01, 0.1, 1, 10], l1_ratio=[.01, .1, .5, .9, .99], max_iter=5000)\n",
    "y_pred_Elastic = _ApplyLinearAlgo(lr, df_tr, df_te, y_train)\n",
    "\n",
    "print(\"\\n\")\n",
    "print (\"\\nRidgeCV\")\n",
    "from sklearn.linear_model import RidgeCV\n",
    "lr=RidgeCV(alphas=[0.0001, 0.0005, 0.001, 0.01, 0.1, 1, 10])\n",
    "y_te_Ridge = _ApplyLinearAlgo(lr, df_tr, df_te, y_train)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"RandomForestRegressor\")\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "lr = RandomForestRegressor()\n",
    "y_te_RF = _ApplyLinearAlgo(lr, df_tr, df_te, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "32698449f0c8050483d9db1194b0edd8ad14381e",
    "collapsed": true
   },
   "source": [
    "With the few submission trials , the ElasticNet gives the best score. So submitting with that. Other algorithms are also giving similar performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fa102eef7a2a09560485c967b22f1ca12822cbb6"
   },
   "outputs": [],
   "source": [
    "idx = pd.read_csv(\"../input/test.csv\").Id\n",
    "my_submission = pd.DataFrame({'Id': idx, 'SalePrice': y_pred_Elastic})\n",
    "# you could use any filename. We choose submission here\n",
    "my_submission.to_csv('submission.csv', index=False)\n",
    "my_submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8749985656d6588d44e116741989b998e0f24b0c"
   },
   "source": [
    "**References:**\n",
    "\n",
    "Took some ideas from the https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
