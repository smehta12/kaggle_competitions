{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is providing the solution for the [Forest Cover Type](https://www.kaggle.com/c/forest-cover-type-kernels-only) competition. This notebook shows study of the severel different type of the ML algorithms to identify the forest trees to predict the forest cover type. The link to on the kaggle for this notebook is https://www.kaggle.com/smehta12/forest-cover-type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import scipy\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3d6b8e377fa3f1a51206a5885deb981ab2bbd1f2"
   },
   "source": [
    "# Data Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(r\"../input/train.csv\")\n",
    "test = pd.read_csv(r\"../input/test.csv\")\n",
    "pd.set_option('display.width', 700)\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3d818c7a2a186c2829e2063c0129074f3ed85d21"
   },
   "outputs": [],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b3ab8f7db45223cc14ee7e2dcfee52fa2b6b1ec6"
   },
   "source": [
    "# Primary Exploration of the DataÂ¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9e99d4675450a88d1220d92fdfaa933c7edf2597"
   },
   "outputs": [],
   "source": [
    "print(train.dtypes)\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ca4a6b7960d6799db13fefe145447e364c121341",
    "collapsed": true
   },
   "source": [
    "By looking at above table, there's no eye catching anomaly in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "43a992ac3f13e97569e0820c09c67fef7c86f702"
   },
   "outputs": [],
   "source": [
    "print(\"The train data shape before any operation on the data: {} \".format(train.shape))\n",
    "print(\"The test data shape before any operation on the data: {} \".format(test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0a9d999c289026fd9f6567d311846bb860c3993c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop Id col as it is not helpful in the classification\n",
    "\n",
    "train.drop(\"Id\", axis=1, inplace=True)\n",
    "test.drop(\"Id\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ba1ea9a1e975b7e82c9f31402a210d21fe301847"
   },
   "source": [
    "# Check for Nans/NA/Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9b4ab95d6440b1215ea368b5f935a69f748fbc9b"
   },
   "outputs": [],
   "source": [
    "# Find out the frequency of nulls in the columns\n",
    "\n",
    "# For training Data\n",
    "count_nans = len(train) - train.count()\n",
    "count_nans = count_nans.to_frame()\n",
    "count_nans.columns=[\"train_nan_count\"]\n",
    "count_nans[\"%_train_nans\"]=(count_nans[\"train_nan_count\"]/train.shape[0]) * 100\n",
    "\n",
    "# For test data\n",
    "count_nans[\"test_nan_count\"] = len(test) - test.count()\n",
    "count_nans[\"%_test_nans\"]=(count_nans[\"test_nan_count\"]/test.shape[0]) * 100\n",
    "\n",
    "count_nans.sort_values(\"train_nan_count\", ascending=False, inplace=True)\n",
    "count_nans.query('train_nan_count > 0 or test_nan_count > 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fec08210b92d5b89885aa7c136b5b85310e024fe"
   },
   "source": [
    "There's no NaNs/NA in any column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "743a5715d6fa01e3d2d06b8a90d2cbbfa6cc203b"
   },
   "outputs": [],
   "source": [
    "# Only for Training data\n",
    "corr_matrix = train.corr().abs()\n",
    "corr_matrix.dropna(axis=1, how=\"all\", inplace=True) # remove the columns which is all Nan\n",
    "corr_matrix.dropna(axis=0, how=\"all\", inplace=True) # remove the rows which is all Nan\n",
    "plt.subplots(figsize=(15,10))\n",
    "sns.heatmap(corr_matrix, cmap=\"jet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f0dad106091d6a0eabfd5c0c4e8f479a8120b40d"
   },
   "source": [
    "By looking at above heatmap following observations can be made:\n",
    "\n",
    "**The shadows at the 9 AM and at 3 PM** : It makes sense because the shadows can be pretty similar at 9 AM and 3 PM at the summaer soltice. Only in the opposite directions. So one of the column can be dropped\n",
    "\n",
    "**Wilderness Area 4 and Elevation**: This correlation doen't makes sense in the real world. It doesn't explain any relationship. So we'll keep those columns as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a045f6e8884b1054eac72cd5b28c920cfe015b4b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.drop(\"Hillshade_3pm\", inplace=True, axis=1)\n",
    "test.drop(\"Hillshade_3pm\", inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "85539ee619c380a62f0a3031fdca403355ef31ef"
   },
   "outputs": [],
   "source": [
    "# take out training target data for further computation\n",
    "y_train = train[\"Cover_Type\"]\n",
    "train.drop([\"Cover_Type\"], inplace=True, axis=1)\n",
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c40ed4afa055e337728ab7690c84cd534a1a5cd9"
   },
   "source": [
    "# Find the skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5f314baf5815496753923ab969cdf0dfa864032e"
   },
   "outputs": [],
   "source": [
    "# Find skewness in the y_train\n",
    "y_train.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "47a466c412330403ccf7a67217dc9ee2917d639b"
   },
   "source": [
    "Based on above plot y is uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0d627f7b375263a8fd9c90975ce97b7d1bfdc506"
   },
   "outputs": [],
   "source": [
    "# Checking Non-Binary columns only\n",
    "skew_check_cols = [\"Elevation\", \"Aspect\", \"Slope\", \"Vertical_Distance_To_Hydrology\", \"Horizontal_Distance_To_Hydrology\", \n",
    "                   \"Horizontal_Distance_To_Roadways\",\"Hillshade_9am\",\"Hillshade_Noon\",\"Horizontal_Distance_To_Fire_Points\" ]\n",
    "train[skew_check_cols].hist(figsize=(25, 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bac93a34eba7d7ccbae5f9ea8cb6465ab9ce99ac",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# skewness = train[skew_check_cols].skew().abs().sort_values(ascending=False)\n",
    "# print(skewness)\n",
    "# skewed_cols = skewness[skewness>0.5].index.tolist()\n",
    "# print(skewed_cols)\n",
    "# train[skewed_cols] = train[skewed_cols].apply(np.log1p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3eb188f78e3c6c1ef2b16cd424c1d23966d40e78",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train[skew_check_cols].skew().abs().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5d07db95e71c8ba15fc31c9f3313162184cf8ae1"
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d2c29023b0e19e54aee001400ff38d00db017eb6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################### Train data #############################################\n",
    "train['HF1'] = train['Horizontal_Distance_To_Hydrology']+train['Horizontal_Distance_To_Fire_Points']\n",
    "train['HF2'] = abs(train['Horizontal_Distance_To_Hydrology']-train['Horizontal_Distance_To_Fire_Points'])\n",
    "train['HR1'] = abs(train['Horizontal_Distance_To_Hydrology']+train['Horizontal_Distance_To_Roadways'])\n",
    "train['HR2'] = abs(train['Horizontal_Distance_To_Hydrology']-train['Horizontal_Distance_To_Roadways'])\n",
    "train['FR1'] = abs(train['Horizontal_Distance_To_Fire_Points']+train['Horizontal_Distance_To_Roadways'])\n",
    "train['FR2'] = abs(train['Horizontal_Distance_To_Fire_Points']-train['Horizontal_Distance_To_Roadways'])\n",
    "train['ele_vert'] = train.Elevation-train.Vertical_Distance_To_Hydrology\n",
    "\n",
    "train['slope_hyd'] = (train['Horizontal_Distance_To_Hydrology']**2+train['Vertical_Distance_To_Hydrology']**2)**0.5\n",
    "train.slope_hyd=train.slope_hyd.map(lambda x: 0 if np.isinf(x) else x) # remove infinite value if any\n",
    "\n",
    "#Mean distance to Amenities \n",
    "train['Mean_Amenities']=(train.Horizontal_Distance_To_Fire_Points + train.Horizontal_Distance_To_Hydrology + train.Horizontal_Distance_To_Roadways) / 3 \n",
    "#Mean Distance to Fire and Water \n",
    "train['Mean_Fire_Hyd']=(train.Horizontal_Distance_To_Fire_Points + train.Horizontal_Distance_To_Hydrology) / 2 \n",
    "\n",
    "####################### Test data #############################################\n",
    "test['HF1'] = test['Horizontal_Distance_To_Hydrology']+test['Horizontal_Distance_To_Fire_Points']\n",
    "test['HF2'] = abs(test['Horizontal_Distance_To_Hydrology']-test['Horizontal_Distance_To_Fire_Points'])\n",
    "test['HR1'] = abs(test['Horizontal_Distance_To_Hydrology']+test['Horizontal_Distance_To_Roadways'])\n",
    "test['HR2'] = abs(test['Horizontal_Distance_To_Hydrology']-test['Horizontal_Distance_To_Roadways'])\n",
    "test['FR1'] = abs(test['Horizontal_Distance_To_Fire_Points']+test['Horizontal_Distance_To_Roadways'])\n",
    "test['FR2'] = abs(test['Horizontal_Distance_To_Fire_Points']-test['Horizontal_Distance_To_Roadways'])\n",
    "test['ele_vert'] = test.Elevation-test.Vertical_Distance_To_Hydrology\n",
    "\n",
    "test['slope_hyd'] = (test['Horizontal_Distance_To_Hydrology']**2+test['Vertical_Distance_To_Hydrology']**2)**0.5\n",
    "test.slope_hyd=test.slope_hyd.map(lambda x: 0 if np.isinf(x) else x) # remove infinite value if any\n",
    "\n",
    "#Mean distance to Amenities \n",
    "test['Mean_Amenities']=(test.Horizontal_Distance_To_Fire_Points + test.Horizontal_Distance_To_Hydrology + test.Horizontal_Distance_To_Roadways) / 3 \n",
    "#Mean Distance to Fire and Water \n",
    "test['Mean_Fire_Hyd']=(test.Horizontal_Distance_To_Fire_Points + test.Horizontal_Distance_To_Hydrology) / 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d5087b7ce6f50f1c34f84565b0899f52d0b99340"
   },
   "source": [
    "# Normalize The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d0eab7f413fe13e3e644728f6d580f14a06eef20"
   },
   "outputs": [],
   "source": [
    "print(train.head(5))\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "train = RobustScaler(with_centering=True, with_scaling=True).fit_transform(train, y_train)\n",
    "test = RobustScaler(with_centering=True, with_scaling=True).fit_transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "69effc1197a3885b9fca007facbc56acb565b181"
   },
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "22e735d9450e726286a6720f555534077c910b4d"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for col in train:\n",
    "    plt.clf()\n",
    "    plt.hist(train, bins='auto')\n",
    "    plt.show()\n",
    "\n",
    "# skew_check_cols = [\"Elevation\", \"Aspect\", \"Slope\", \"Vertical_Distance_To_Hydrology\", \"Horizontal_Distance_To_Hydrology\", \n",
    "#                    \"Horizontal_Distance_To_Roadways\",\"Hillshade_9am\",\"Hillshade_Noon\",\"Horizontal_Distance_To_Fire_Points\" ]\n",
    "# train[skew_check_cols].hist(figsize=(25, 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0f0fdaacb4f5a74a25f07ffccb78a751b0cac708",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9db9fddcfbb2361b71577f7f94562203e1ea1934"
   },
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2229e7fe328efc070637dc03571c0e728a6df07a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "def kfold_cv(train, model, y_true):\n",
    "    accuracy = []\n",
    "    kfold = StratifiedKFold(n_splits=10, random_state=None)\n",
    "    for tr, te in kfold.split(train, y_true):\n",
    "        model.fit(train.iloc[tr], y_true.iloc[tr])\n",
    "        predictions = model.predict(train.iloc[te])\n",
    "        accuracy.append(accuracy_score(predictions, y_true.iloc[te]))\n",
    "    \n",
    "    print(\"KFold Accuracy:{}\".format(np.mean(accuracy)))\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    #acc_score = accuracy_score(y_true, y_pred)\n",
    "    print(\"accuracy score:{}\".format(accuracy_score(y_true, y_pred)))\n",
    "    print(\"f1 score:{}\".format(f1_score(y_true, y_pred, average=\"micro\")))\n",
    "    # ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "59001fc64c133f94f3d2a2792e0df2351b340a9e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# TODO: Try multiscoring\n",
    "def _grid_search(model, param_grid, train, y_train):\n",
    "    clf = GridSearchCV(model, param_grid, cv=10, scoring='accuracy', refit=True)\n",
    "    clf.fit(train, y_train)\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print(clf.best_params_)\n",
    "    print(\"Grid scores on development set:\")\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"% (mean, std * 2, params))\n",
    "    print(clf.scorer_)\n",
    "    print(clf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bf6e3140ed7c6d3f955ebd8a8db43dc6936fff18"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "print(\"Logistic_Regression\")\n",
    "lr = LogisticRegression()\n",
    "lr.fit(train, y_train)\n",
    "y_train_pred = lr.predict(train)\n",
    "y_test_pred = lr.predict(test)\n",
    "\n",
    "kfold_cv(train, lr, y_train)\n",
    "calculate_metrics(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "20702487e7c32888899bb2430b0b73765321d1ca",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "print(\"Gradient Boost\")\n",
    "cl = GradientBoostingClassifier(n_estimators=200, learning_rate=0.1)\n",
    "cl.fit(train, y_train)\n",
    "y_train_pred = cl.predict(train)\n",
    "y_test_pred = cl.predict(test)\n",
    "\n",
    "# print(\"classifier score {}\".format(cl.score(train, y_train)))\n",
    "\n",
    "kfold_cv(train, cl, y_train)\n",
    "calculate_metrics(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "58f3a646fa8690c96efac2d83e6631fd66b31767",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# orig_stdout=sys.stdout\n",
    "# sys.stdout=open('output.txt', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7eca5a2e896cbecf525291e97f116a8a5bb3b47d"
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# d_train = lgb.Dataset(train, label=y_train)\n",
    "# params = {}\n",
    "# params['learning_rate'] = 0.003\n",
    "# params['boosting_type'] = 'gbdt'\n",
    "# params['objective'] = 'binary'\n",
    "# params['metric'] = 'binary_logloss'\n",
    "# params['sub_feature'] = 0.5\n",
    "# params['num_leaves'] = 10\n",
    "# params['min_data'] = 50\n",
    "# params['max_depth'] = 10\n",
    "\n",
    "# print(\"lgb_classifier\")\n",
    "# param_grid = {'n_estimators':[500, 600, 800], 'learning_rate':[0.1], 'max_depth':[40, 60], 'num_leaves':[127, 255], 'boosting_type':['gbdt']}\n",
    "# cl = lgb.LGBMClassifier()\n",
    "# _grid_search(cl, param_grid, train, y_train)\n",
    "\n",
    "cl = lgb.LGBMClassifier(boosting_type='gbdt', learning_rate=0.1, max_depth=40, n_estimators=500, num_leaves=63, objective=\"multiclass\")\n",
    "cl.fit(train, y_train)\n",
    "y_train_pred = cl.predict(train)\n",
    "y_test_pred = cl.predict(test)\n",
    "\n",
    "kfold_cv(train, cl, y_train)\n",
    "calculate_metrics(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eee4274fac15d9b7f4982de5143da086d84fa415",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "42df05c965d270a237f2b182426bce298d5273e2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "print(\"xgb_classifier\")\n",
    "param_grid = {'n_estimators':[70, 300, 500], 'learning_rate':[0.001, 0.01, 0.1], 'max_depth':[2, 10, 20]}\n",
    "cl = XGBClassifier(n_jobs=10)\n",
    "_grid_search(cl, param_grid, train, y_train)\n",
    "\n",
    "cl = XGBClassifier(n_jobs=20, n_estimators=1000, learning_rate=0.1, max_depth=100)\n",
    "cl.fit(train, y_train)\n",
    "y_train_pred = cl.predict(train)\n",
    "y_test_pred = cl.predict(test)\n",
    "\n",
    "# print(\"classifier score {}\".format(cl.score(train, y_train)))\n",
    "\n",
    "kfold_cv(train, cl, y_train)\n",
    "calculate_metrics(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fd35b4fa95f9ed37df05e296aae4c27f1f9f27a6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "cl = ExtraTreesClassifier(n_estimators=400)#, oob_score=True, bootstrap=True)\n",
    "cl.fit(train, y_train)\n",
    "y_train_pred = cl.predict(train)\n",
    "y_test_pred = cl.predict(test)\n",
    "\n",
    "#print(\"oob score {}\".format(cl.oob_score_))\n",
    "\n",
    "kfold_cv(train, cl, y_train)\n",
    "calculate_metrics(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3942cdc8d513a0b15f3e2e2c400eca08aaf9f2b4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = pd.read_csv(\"../input/test.csv\").Id\n",
    "my_submission = pd.DataFrame({'Id': idx, 'Cover_Type': y_test_pred})\n",
    "# you could use any filename. We choose submission here\n",
    "my_submission.to_csv('submission.csv', index=False)\n",
    "my_submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
